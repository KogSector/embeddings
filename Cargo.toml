[package]
name = "embeddings"
version = "0.2.0"
edition = "2021"
authors = ["KogSector"]
description = "A high-performance embedding microservice using local ONNX models"
license = "MIT"
repository = "https://github.com/KogSector/embeddings"
keywords = ["embeddings", "rag", "nlp", "vector", "semantic-search", "onnx"]
categories = ["text-processing", "web-programming", "machine-learning"]

[[bin]]
name = "embeddings"
path = "src/main.rs"

[lib]
name = "embeddings"
path = "src/lib.rs"

[dependencies]
# Async runtime
tokio = { version = "1.35", features = ["full"] }

# HTTP framework
axum = { version = "0.7", features = ["json", "macros"] }
tower = { version = "0.4", features = ["util", "timeout"] }
tower-http = { version = "0.5", features = ["cors", "trace"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# UUID generation
uuid = { version = "1.6", features = ["v4", "serde"] }

# Date/time
chrono = { version = "0.4", features = ["serde"] }

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Logging and tracing
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }

# Async traits
async-trait = "0.1"

# Configuration
dotenvy = "0.15"

# Hashing for cache keys
sha2 = "0.10"

# Parallel processing for vector operations
rayon = "1.8"

# ============================================
# Local Model Inference Dependencies
# ============================================

# ONNX Runtime for model inference
ort = { version = "2.0.0-rc.10", features = ["load-dynamic"] }

# HuggingFace Tokenizers for text tokenization
tokenizers = { version = "0.19", default-features = false, features = ["onig"] }

# N-dimensional arrays for tensor operations
ndarray = "0.15"

# LRU Cache for embedding caching
lru = "0.12"

[dev-dependencies]
tokio-test = "0.4"
pretty_assertions = "1.4"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
